{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPCeI+LW/14TldGYfvZl997",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JamilaAferhane/Document_Summarization/blob/main/document_summarization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HbNsbVnzkgHR"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W-i1HfDKsLMW",
        "outputId": "dd0bbacd-2ae5-4538-ffed-a39ffefd6be5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install PyPDF2"
      ],
      "metadata": {
        "id": "KEd9t3o7BmfL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing necessary modules"
      ],
      "metadata": {
        "id": "zlKbsxkOzGNF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#importation of necessary libraries\n",
        "import os.path\n",
        "import nltk\n",
        "from gensim import corpora\n",
        "from gensim.models import LsiModel\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "import PyPDF2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MiB2ymAOskG8",
        "outputId": "e742db6f-c36b-425a-98bc-a4aeec762da1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Converting pdf to text"
      ],
      "metadata": {
        "id": "6fRT36KUQ7oo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_pdf_to_txt(pdf_path, txt_path):\n",
        "    with open(pdf_path, 'rb') as pdf_file:\n",
        "        pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
        "        text_content = ''\n",
        "\n",
        "        # Find the start and end pages for each section\n",
        "        abstract_start_page = None\n",
        "        intro_start_page = None\n",
        "        conclusion_start_page = None\n",
        "        references_start_page = None\n",
        "\n",
        "        for i, page in enumerate(pdf_reader.pages):\n",
        "            page_text = page.extract_text()\n",
        "\n",
        "            if not abstract_start_page and ('Abstract' in page_text or 'ABSTRACT' in page_text ):\n",
        "                abstract_start_page = i\n",
        "                print ('I', i)\n",
        "\n",
        "            if not intro_start_page and ('Introduction' in page_text or 'INTRODUCTION' in page_text):\n",
        "                intro_start_page = i\n",
        "                print('II', i)\n",
        "\n",
        "            if not conclusion_start_page and ('Conclusion' in page_text or 'CONCLUSION' in page_text ):\n",
        "                conclusion_start_page = i\n",
        "                print('III', i)\n",
        "\n",
        "            if not references_start_page and ('References' in page_text or 'REFERENCES' in page_text or 'Bibliography' in page_text or  'BIBLIOGRAPHY' in page_text ):\n",
        "                references_start_page = i\n",
        "                print('IIII', i)\n",
        "\n",
        "        # Determine the start and end pages for each section\n",
        "        start_page = None\n",
        "        end_page = None\n",
        "\n",
        "        if intro_start_page is not None:\n",
        "            start_page = intro_start_page\n",
        "            print('a')\n",
        "\n",
        "        elif abstract_start_page is not None:\n",
        "            start_page = abstract_start_page\n",
        "            print('b')\n",
        "\n",
        "\n",
        "\n",
        "        if conclusion_start_page is not None:\n",
        "            end_page = conclusion_start_page\n",
        "            print('c')\n",
        "            print('end page', end_page)\n",
        "\n",
        "        elif references_start_page is not None:\n",
        "            end_page = references_start_page\n",
        "            print('d')\n",
        "\n",
        "        # Extract the text based on the determined page range\n",
        "        for i, page in enumerate(pdf_reader.pages):\n",
        "\n",
        "            if (start_page is not None) and (i >= start_page) and (end_page is None or i < end_page):\n",
        "                print('Entering the considered part of the pdf')\n",
        "\n",
        "                if i == start_page : #and start_page != 0  :\n",
        "\n",
        "\n",
        "                    page_text = page.extract_text()\n",
        "\n",
        "                    if ('INTRODUCTION' in page_text):\n",
        "                        text_content += page_text.split(\"INTRODUCTION\",1)[1]\n",
        "                        print('Everthing before \\'Introduction\\' is not included')\n",
        "\n",
        "                    elif ('Introduction' in page_text):\n",
        "                        text_content += page_text.split(\"Introduction\",1)[1]\n",
        "                        print('Everthing before \\'Introduction\\': is not included')\n",
        "\n",
        "                    elif ('Abstract' in page_text):\n",
        "                        text_content += page_text.split(\"Abstract\",1)[1]\n",
        "                        print('Everthing before \\'Abstract\\' is not included')\n",
        "\n",
        "                    elif ('ABSTRACT' in page_text):\n",
        "                        text_content += page_text.split(\"ABSTRACT\",1)[1]\n",
        "                        print('Everthing before \\'Abstract\\': is not included')\n",
        "\n",
        "\n",
        "                else:\n",
        "                    text_content += page.extract_text()\n",
        "                    print('A whole page is added')\n",
        "\n",
        "            elif ( start_page is None and (end_page is None or i<end_page)):\n",
        "                text_content += page.extract_text()\n",
        "\n",
        "            if i == end_page:\n",
        "\n",
        "                page_text = page.extract_text()\n",
        "\n",
        "                if any(keyword in page_text for keyword in ('Conclusion', 'CONCLUSION')):\n",
        "                    print (\"there is a conclusion\")\n",
        "                    conclusion_start_index = page_text.index(keyword)\n",
        "                    # Find the index of the next title after the conclusion\n",
        "                    next_title_index = -1\n",
        "                    for title in ['Acknowledgements','ACKNOWLEDGEMENTS','Acknowledgement','ACKNOWLEDGEMENT', 'Acknowledgments'\n",
        "                                  ,'ACKNOWLEDGMENTS','Acknowledgment','ACKNOWLEDGMENT', 'Appendix', 'APPENDIX','References',\n",
        "                                  'REFERENCES', 'Bibliography', 'BIBLIOGRPHY', 'Works Cited', 'WORKS CITED', 'Appendices',\n",
        "                                  'APPENDICES', 'Glossary', 'GLOSSARY','Index', 'List of Figures', 'List of Tables',\n",
        "                                  'About the Author(s)', 'Notes', 'INDEX', 'LIST OF FIGURES', 'LIST OF TABLES',\n",
        "                                  'ABOUT THE AUTHOR(S)', 'NOTES', 'About the author', 'ABOUT THE AUTHOR',\n",
        "                                  'acknowledgements', 'acknowledgements', 'acknowledgement', 'acknowledgement',\n",
        "                                  'acknowledgments','acknowledgments', 'acknowledgment', 'acknowledgment', 'appendix',\n",
        "                                  'appendix', 'references','references', 'bibliography', 'bibliography', 'works cited',\n",
        "                                  'works cited', 'appendices', 'appendices', 'glossary', 'glossary', 'index',\n",
        "                                  'list of figures', 'list of tables', 'about the author(s)', 'notes', 'index',\n",
        "                                  'list of figures', 'list of tables', 'about the author(s)', 'notes', 'about the author']:\n",
        "                        #conclusion_start_index: The starting index from which the search should begin within\n",
        "                        title_index = page_text.find(title, conclusion_start_index)\n",
        "                        print(title, title_index)\n",
        "                        if title_index != -1 and (next_title_index == -1 or title_index < next_title_index):\n",
        "                            print(title)\n",
        "                            next_title_index = title_index\n",
        "                    if conclusion_start_index != -1:\n",
        "                        conclusion_content = page_text[conclusion_start_index:next_title_index].strip()\n",
        "                        text_content += conclusion_content\n",
        "                    else:\n",
        "                        text_content += page_text[conclusion_start_index:]\n",
        "\n",
        "                elif ('REFERENCES' in page_text):\n",
        "                    text_content += page_text.split(\"References\",1)[0]\n",
        "                    print('There is a references part but it is not considered')\n",
        "\n",
        "                elif ('References' in page_text):\n",
        "                    text_content += page_text.split(\"References\",1)[0]\n",
        "                    print('There is a references part but it is not considered')\n",
        "\n",
        "                elif ('Bibliography' in page_text or 'BIBLIOGRAPHY'):\n",
        "                    text_content += page_text.split(\"Bibliography\",1)[0]\n",
        "                    print('There is a bibliography part but it is not considered')\n",
        "                elif ('BIBLIOGRAPHY' in page_text ):\n",
        "                    text_content += page_text.split(\"Bibliography\",1)[0]\n",
        "                    print('There is a bibliography part but it is not considered')\n",
        "\n",
        "        cleaned_text = text_content.strip()  # Remove leading/trailing whitespaces\n",
        "        cleaned_text = cleaned_text.replace('\\n', ' ')  # Replace all line breaks with spaces\n",
        "        cleaned_text = cleaned_text.replace('. ', '.\\n')  # Add line breaks after periods\n",
        "\n",
        "        with open(txt_path, 'w', encoding='utf-8') as txt_file:\n",
        "            txt_file.write(cleaned_text)\n",
        "\n",
        "\n",
        "# Usage example\n",
        "# pdf_path = './pdf_for_test/test_7.pdf'\n",
        "# txt_path = './generated_texts/test_s.txt'\n",
        "# convert_pdf_to_txt(pdf_path, txt_path)\n",
        "\n",
        "\n",
        "# def convert_pdf_to_txt(pdf_path, txt_path):\n",
        "#     with open(pdf_path, 'rb') as pdf_file:\n",
        "#         pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
        "#         text_content = ''\n",
        "\n",
        "#         for page in pdf_reader.pages:\n",
        "#             text_content += page.extract_text()\n",
        "\n",
        "#         with open(txt_path, 'w', encoding='utf-8') as txt_file:\n",
        "#             txt_file.write(text_content)"
      ],
      "metadata": {
        "id": "8AZEkUFERAO6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading the document"
      ],
      "metadata": {
        "id": "VK24HYsfzQLw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " # Load the document\n",
        "def convert_to_sentences(text):\n",
        "    \"\"\"\n",
        "    Input  : text document\n",
        "    Purpose: convert text into a list of sentences\n",
        "    Output : list of sentences\n",
        "    \"\"\"\n",
        "    sentences = sent_tokenize(text)\n",
        "    return sentences\n",
        "\n",
        "def load_data(path, file_name):\n",
        "    \"\"\"\n",
        "    Input  : path and file_name\n",
        "    Purpose: loading text file\n",
        "    Output : list of paragraphs/documents and\n",
        "             title (initial 100 characters considered as the title of each document)\n",
        "    \"\"\"\n",
        "    documents_list = []\n",
        "    with open(os.path.join(path, file_name), \"r\") as fin:\n",
        "        text = fin.read()\n",
        "        sentences = convert_to_sentences(text)\n",
        "        # Split the document into sentences\n",
        "        for sentence in sentences:\n",
        "            # Append the current sentence to the current document\n",
        "            documents_list.append(sentence)\n",
        "            # Extract the first 100 characters (or less) as the title\n",
        "    print(\"Total Number of Sentences:\", len(documents_list))\n",
        "    return documents_list\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Example usage\n",
        "\n",
        "# pdf_path = '/content/drive/My Drive/test.pdf'\n",
        "# txt_path = '/content/drive/My Drive/test.txt'\n",
        "# convert_pdf_to_txt(pdf_path, txt_path)\n",
        "# document_list = load_data(\"\",\"/content/drive/My Drive/test.txt\")\n",
        "# print(document_list)\n",
        "\n"
      ],
      "metadata": {
        "id": "4c9T8Zu3tGNI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "661ed61e-88ac-408f-ef16-35e537dd2ef1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I 1\n",
            "II 1\n",
            "IIII 10\n",
            "a\n",
            "d\n",
            "Entering the considered part of the pdf\n",
            "Everthing before 'Introduction': is not included\n",
            "Entering the considered part of the pdf\n",
            "A whole page is added\n",
            "Entering the considered part of the pdf\n",
            "A whole page is added\n",
            "Entering the considered part of the pdf\n",
            "A whole page is added\n",
            "Entering the considered part of the pdf\n",
            "A whole page is added\n",
            "Entering the considered part of the pdf\n",
            "A whole page is added\n",
            "Entering the considered part of the pdf\n",
            "A whole page is added\n",
            "Entering the considered part of the pdf\n",
            "A whole page is added\n",
            "Entering the considered part of the pdf\n",
            "A whole page is added\n",
            "There is a references part but it is not considered\n",
            "Total Number of Sentences: 86\n",
            "['Latent Semantic Analysis (LSA) is a technique for creating vector-based representations of texts which are claimed to capture their semantic con- tent.', 'The primary function of LSA is to compute the similarity of text pairs 1by comparing their vector representations.', 'This relatively simple similarity metric has been situated within a psychological theory of text meaning and has been shown to closely match human capabilities on a variety of tasks.', 'This article follows the developmental path of LSA, describing its histor- ical context, showing how it computes and uses its vector representations, and then giving examples of the theoretical and empirical support for LSA and its current research directions.', '2 How LSA works LSA (originally known as Latent Semantic Indexing) was developed for the task of Information Retrieval, that is, selecting from a large database of documents a few relevant documents which match a given query .', 'Previ- ous approaches to this task included keyword-matching, weighted keyword matching, and vector-based representations based on the occurrence of words in documents.', 'LSA extends the vector-based approach by using Singular Value Decomposition (SVD) to reconﬁgure the data.', 'The details of this pro- cess are described below, but the intuition is that there is a set of underlying latent variables which spans the meanings that can be expressed in a partic- ular language.', 'These variables are assumed to be independent (and therefore orthogonal in the vector space).', 'SVD is a matrix algebra technique which essentially re-orients and ranks the dimensions in a vector space.', 'Because the dimensions in a vector space computed by SVD are ordered from most to least signiﬁcant, if some of the less signiﬁcant dimensions are 2ignored, the reduced representation is guaranteed to be the best possible for that dimensionality.', 'In LSA, the typical assumption is that only the top 300 or so dimensions (out of tens or even hundreds of thousands) are useful for capturing the meaning of texts.', 'By basing the representations on a reduced number of dimensions, words that generally occur in similar contexts have similar vectors and will therefore get a high similarity rating.', 'The discarded dimensions are assumed to be the product of noise, random associations, or some other non-essential factor.', 'That LSA performed information retrieval better than its rival approaches is not especially surprising.', 'What is more surprising is how well it models human behavior on a variety of linguistic tasks.', 'Before describing these, however, the LSA technique is described in more detail.', 'Although there are some variations, the most common steps are these: •Collect a large set of (domain-relevant) text and separate it into “docu- ments”.', 'For most applications, each paragraph is treated as a separate document based on the intuition that the information within a para- graph tends to be coherent and related.', '•Next, create a co-occurrence matrix of documents and terms.', 'The cell in this matrix corresponding to document xand term ycontains the number of times yoccurs in x.', 'A term is deﬁned as a word which occurs in more than one document, and no stemming or other morphological analysis is performed to try to combine diﬀerent forms of the same word.', '3If there are mterms and ndocuments, this matrix can be viewed as giving a representation which has an m-dimensional vector for each document, and an n-dimensional vector for each term.', '•The values in each cell may be weighted to reduce the eﬀect of common words that occur through the corpus.', 'A common weighting method is “log entropy”, based on Information Theory, which multiplies the value by its information gain.', '•SVD is invoked with a parameter kwhich speciﬁes the desired number of dimensions.', '(In principle, the SVD would be computed with all the dimensions to create three matrices that, when multiplied together, would produce the original data, but due to the amount of memory that this would require, this is not feasible.', 'Instead, the currently used algorithms are optimized for dealing with sparse data spaces and compute only the most signiﬁcant kdimensions of the matrices.)', 'The result of the processing above is three matrices.', 'One has a k- dimensional vector for each document, one has a k-dimensional vector for each term in the corpus, and one has the ksingular values.', 'The ﬁrst two matrices deﬁne two diﬀerent vector spaces which are also diﬀerent from the space deﬁned by the original matrix.', 'The singular values can be used to transform a vector from one space to the other.', 'The use of these matrices depends on the application.', 'For information retrieval, the document vectors contain the LSA repre- sentation of each document.', 'A query is turned into a “pseudodoc” in the 4document vector space by combining the vectors for the terms in the query, and dividing by the singular values.', 'Vectors are typically compared by com- puting the cosine between them.', '(Some applications use other distance met- rics.)', 'The closest vectors from the document vector space correspond to the documents which are closest in meaning to the query (according to LSA).', 'In most other applications, the original documents are only used for train- ing, that is, creating the semantic space.', 'To compare new texts, the term vectors are combined as described above.', 'Here, no manipulation with the singular values is required because the vectors are compared in the term space.', 'For more details about the mathematical foundations of LSA, see Golub (1989) and Hu (2005).', 'For more details about the creation of LSA spaces, see Deerwester (1990) and Quesada (2005).', '3 Support for LSA Support for LSA might be said to stem from the time of World War II when Wittgenstein wrote (what was later translated as): .', '.', '.', 'for a large class of cases — though not for all — in which we employ the word “meaning” it can be deﬁned as thus, the meaning of the word is its use in language.', '(Wittgenstein, 1958, p.\\n20) 5There have been a large number of psychological studies which have taken Wittgenstein’s words to heart, and shown that LSA’s behavior is closely matched with that of humans, for example: •LSA acquires words at a similar pace to human children, sometimes exceeding the number of words to which it is exposed (see Landauer and Dumais, 1997) .', '•LSA’s knowledge of synonyms is as good as that of second-language English speakers as evidenced by scores on the Test of English as a Foreign Language (TOEFL, see Landauer, 1997) .', '•LSA can tell students what they should read next to help them learn (see Wolfe, 1998).', '•LSA can even interpret metaphors like, “My lawyer is a shark” (see Kintsch, 2001).', 'For textual applications, LSA has another beneﬁt besides its high cor- relation with human behavior.', 'When compared with the traditional labor- intensive approach to Natural Language Processing — developing a gram- mar, a lexicon, a semantic representation and the processing engine needed to combine them — developing an LSA-based representation is quite simple.', 'It also has the advantage of graceful degradation.', 'If it doesn’t know a word, LSA simply ignores it and bases its representation on the other words.', 'This has led researchers to use LSA for a variety of applications, including: 6•intelligent tutoring systems which allow students to enter unconstrained natural language replies to questions (see Graesser, 2000 and Wiemer- Hastings, 2004), •grading psychology essays by comparing them to pre-graded essays (see Foltz, 1996) , •evaluating summaries of documents to help teach summarization skills (Summary Street, described at http://colit.org/), •helping students learn to properly integrate and cite material from multiple documents (see Britt, 2005), and •evaluating airplane landings in a ﬂight simulator (see Quesada, 2005).', 'The only applicability constraints for LSA are that the task is text-based, it can be framed in terms of computing the similarity of texts, and there is an available training corpus.', 'The tutoring systems, for example, compare a student’s answer for a question to a set of expected answers.', 'If the student’s response is close enough to a good answer, then the system gives positive feedback and moves on to the next question.', 'If the student’s answer matches an expected bad answer, then the system steers the student back on track.', '4 Issues The research issues facing the LSA community range from the practical to the philosophical.', 'One basic question addresses the size and substance of the 7training corpus.', 'Many eﬀective LSA applications have been developed us- ing relatively small corpora.', 'In one of the successful applications mentioned above, LSA was trained on a corpus of only a couple hundred kilobytes with 2000 word types, 30,000 word tokens, and 325 documents.', 'In contrast, re- searchers at the University of Colorado have reported that they have trained LSA on a corpus containing 750,000 word types, 550 million word tokens, and 3.6 million documents.', 'Unfortunately, there is little hard evidence on what the “ideal” size of an LSA corpus might be.', 'The current data suggests that adding additional texts is unlikely to reduce performance, so a basic rule of thumb is, “the more the better.” The obvious follow-up question is, “What kinds of text should be included in an LSA corpus?” The common wisdom holds that the corpus should consist of texts which are relevant to the particular target task.', 'The domain can deﬁne a sub-language where words are interpreted in consistent ways.', 'Furthermore, a primary concern is to achieve suﬃcient coverage of the words which will be encountered in the course of running the application.', 'A domain- speciﬁc corpus will have a higher percentage of relevant words and will thus not waste its “representational power” on words that will not be seen by the application.', 'One critical objection that is raised against the LSA approach is that not only does it ignore the syntactic structure of sentences, it even ignores word order.', 'In other words, LSA treats a text as a bag of words.', 'In practice, LSA does well with longer passages of words (deﬁned by Rehder (1998) as 8more than 200 words ) where syntactic details may be “washed out”, and it also does well with single words (the TOEFL test, for example), but it does not do well on single sentences as shown by Wiemer-Hastings (1999) .', 'There have been a variety of approaches which attempt to deal with this, including surface parsing of sentences so that the components can be compared sepa- rately with LSA and using String Edit Theory to infer structural relations (see Wiemer-Hastings, 2001, Kanejiya, 2003, and Dennis, 2005) .', 'Another notable gap in LSA’s competence is negations.', 'thing that LSA “ignores” is negations, either because they are omitted from the LSA training via a “stop words” list, or simply because their widespread use throughout a corpus renders them representationally depleted.', 'Although no satisfactory approach yet exists for dealing with negations, a possibility would be to treat them as an essentially syntactic component that can processed as described above.', 'A more fundamental question about LSA is what its dimensions “mean”.', 'Because they represent latent variables, there is no clear deﬁnition.', 'As shown by Hu (2003), there is a high correlation between the ﬁrst dimension and the frequency of occurrence of the words in the corpus .', 'Beyond that, there are no clear answers.', 'There is also considerable debate as to what extent LSA captures ﬁrst order co-occurrence or higher order co-occurrence.', 'Recent evidence from Denhi` ere (2005) shows that although second order eﬀects do occur, large changes in the similarity measure between two words can be seen when a document is added to a training corpus in which both words occur (ﬁrst order co-occurrence) .', '9LSA’s utility and correspondence with human behavor have made it a popular technique for psycholinguistic research and text processing.', 'The issues describe here (along with others, for example, “Is LSA at all psycho- logically plausible?”) will keeep researchers busy for years to come.', 'A col- lection edited by McNamara (2005) provides considerably more detail about the current practice and research on LSA.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing the document"
      ],
      "metadata": {
        "id": "cdsKs__bzYPp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess the document\n",
        "def preprocess_data(doc_set):\n",
        "    \"\"\"Input  : document list\n",
        "    Purpose: preprocess text (tokenize, removing stopwords, and stemming)\n",
        "    Output : preprocessed text \"\"\"\n",
        "    # initialize regex tokenizer\n",
        "    tokenizer = RegexpTokenizer(r'\\w+')\n",
        "    # create English stop words list\n",
        "    en_stop = set(stopwords.words('english'))\n",
        "    # Create p_stemmer of class PorterStemmer\n",
        "    p_stemmer = PorterStemmer()\n",
        "    # list for tokenized documents in loop\n",
        "    texts = []\n",
        "    # loop through document list\n",
        "    for i in doc_set:\n",
        "        # clean and tokenize document string\n",
        "        raw = i.lower()\n",
        "        tokens = tokenizer.tokenize(raw)\n",
        "        # remove stop words from tokens\n",
        "        stopped_tokens = [i for i in tokens if not i in en_stop]\n",
        "        # stem tokens\n",
        "        stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
        "        # add tokens to list\n",
        "        texts.append(stemmed_tokens)\n",
        "    return texts"
      ],
      "metadata": {
        "id": "-pllTQ2GvjQq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extracting keywords"
      ],
      "metadata": {
        "id": "veDYCGk0n-K6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_keywords(text, num_keywords=5):\n",
        "\n",
        "    # Flatten the list of lists into a single list of tokens\n",
        "    tokens = [token for sublist in text for token in sublist]\n",
        "\n",
        "    tf_vectorizer = CountVectorizer()\n",
        "    tfidf_transformer = TfidfTransformer()\n",
        "\n",
        "    tf_matrix = tf_vectorizer.fit_transform([' '.join(tokens)])\n",
        "    tfidf_matrix = tfidf_transformer.fit_transform(tf_matrix)\n",
        "\n",
        "    feature_names = tf_vectorizer.get_feature_names_out()\n",
        "    tfidf_scores = tfidf_matrix.toarray()[0]\n",
        "\n",
        "    keywords = [feature_names[idx] for idx in tfidf_scores.argsort()[-num_keywords:][::-1]]\n",
        "\n",
        "    return keywords\n"
      ],
      "metadata": {
        "id": "7ZQGLYAzoC6i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparing the term frequency matrix of the document"
      ],
      "metadata": {
        "id": "wul5uGkuz1du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_corpus(doc_clean):\n",
        "    \"\"\"\n",
        "    Input  : clean document\n",
        "    Purpose: create term dictionary of our courpus and Converting list of documents (corpus) into Document Term Matrix\n",
        "    Output : term dictionary and Document Term Matrix\n",
        "    \"\"\"\n",
        "    # Creating the term dictionary of our courpus, where every unique term is assigned an index. dictionary = corpora.Dictionary(doc_clean)\n",
        "    dictionary = corpora.Dictionary(doc_clean)\n",
        "    # Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n",
        "    doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
        "    # generate LDA model\n",
        "    return dictionary,doc_term_matrix"
      ],
      "metadata": {
        "id": "8TD2s18lw8DZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create a gensim latent semantic"
      ],
      "metadata": {
        "id": "Yw2j11w11Poi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_gensim_lsa_model(doc_clean,number_of_topics,words):\n",
        "    \"\"\"\n",
        "    Input  : clean document, number of topics and number of words associated with each topic\n",
        "    Purpose: create LSA model using gensim\n",
        "    Output : return LSA model\n",
        "    \"\"\"\n",
        "    dictionary,doc_term_matrix=prepare_corpus(doc_clean)\n",
        "    # generate LSA model\n",
        "    lsamodel = LsiModel(doc_term_matrix, num_topics=number_of_topics, id2word = dictionary)  # train model\n",
        "    print(lsamodel.print_topics(num_topics=number_of_topics, num_words=words))\n",
        "    return lsamodel"
      ],
      "metadata": {
        "id": "8h5ZXHzI1R7T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Select the number of topics the documents have to be divided into"
      ],
      "metadata": {
        "id": "sM34PnU46S7s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "number_of_topics=2\n",
        "words=20\n",
        "\n",
        "pdf_path = '/content/drive/My Drive/test.pdf'\n",
        "txt_path = '/content/drive/My Drive/test.txt'\n",
        "convert_pdf_to_txt(pdf_path, txt_path)\n",
        "document_list=load_data(\"\",\"/content/drive/My Drive/test.txt\")\n",
        "clean_text=preprocess_data(document_list)\n",
        "keywords = extract_keywords(clean_text)\n",
        "dict1,doc_term_matrix=prepare_corpus(clean_text)\n",
        "model=create_gensim_lsa_model(clean_text,number_of_topics,words)\n",
        "corpus_lsi = model[doc_term_matrix]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "miBt1rsd55Yt",
        "outputId": "37c4dcf9-efa4-48c9-93fc-41a440d7d1b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I 1\n",
            "II 1\n",
            "IIII 10\n",
            "a\n",
            "d\n",
            "Entering the considered part of the pdf\n",
            "Everthing before 'Introduction': is not included\n",
            "Entering the considered part of the pdf\n",
            "A whole page is added\n",
            "Entering the considered part of the pdf\n",
            "A whole page is added\n",
            "Entering the considered part of the pdf\n",
            "A whole page is added\n",
            "Entering the considered part of the pdf\n",
            "A whole page is added\n",
            "Entering the considered part of the pdf\n",
            "A whole page is added\n",
            "Entering the considered part of the pdf\n",
            "A whole page is added\n",
            "Entering the considered part of the pdf\n",
            "A whole page is added\n",
            "Entering the considered part of the pdf\n",
            "A whole page is added\n",
            "There is a references part but it is not considered\n",
            "Total Number of Sentences: 86\n",
            "[(0, '0.473*\"lsa\" + 0.406*\"word\" + 0.255*\"document\" + 0.249*\"see\" + 0.151*\"vector\" + 0.135*\"2005\" + 0.132*\"corpu\" + 0.122*\"use\" + 0.119*\"represent\" + 0.106*\"applic\" + 0.105*\"student\" + 0.103*\"text\" + 0.097*\"similar\" + 0.094*\"help\" + 0.086*\"base\" + 0.086*\"grade\" + 0.086*\"summari\" + 0.086*\"evalu\" + 0.086*\"essay\" + 0.086*\"languag\"'), (1, '-0.424*\"word\" + 0.341*\"see\" + 0.197*\"student\" + 0.174*\"help\" + 0.169*\"2005\" + 0.166*\"evalu\" + 0.166*\"summari\" + 0.166*\"grade\" + 0.166*\"essay\" + -0.123*\"lsa\" + -0.121*\"corpu\" + 0.100*\"system\" + -0.099*\"similar\" + -0.093*\"vector\" + -0.092*\"represent\" + 0.091*\"learn\" + 0.090*\"compar\" + 0.089*\"tutor\" + 0.089*\"varieti\" + 0.088*\"quesada\"')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for doc, as_text in zip(corpus_lsi, document_list):\n",
        "    print(doc, as_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F525WfON6W6f",
        "outputId": "53ab3c2b-7430-4ddc-fc08-9897ff4a9513"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(0, 1.153840519596598), (1, -0.5633294910394763)] Latent Semantic Analysis (LSA) is a technique for creating vector-based representations of texts which are claimed to capture their semantic con- tent.\n",
            "[(0, 1.1046627524030228), (1, -0.43781962012493486)] The primary function of LSA is to compute the similarity of text pairs 1by comparing their vector representations.\n",
            "[(0, 0.7560582240652032), (1, -0.2889067628937494)] This relatively simple similarity metric has been situated within a psychological theory of text meaning and has been shown to closely match human capabilities on a variety of tasks.\n",
            "[(0, 1.8813663627116441), (1, -0.3990646271206591)] This article follows the developmental path of LSA, describing its histor- ical context, showing how it computes and uses its vector representations, and then giving examples of the theoretical and empirical support for LSA and its current research directions.\n",
            "[(0, 2.0375292895239103), (1, -0.37372896615587753)] 2 How LSA works LSA (originally known as Latent Semantic Indexing) was developed for the task of Information Retrieval, that is, selecting from a large database of documents a few relevant documents which match a given query .\n",
            "[(0, 1.5814615923125204), (1, -0.8565792965495629)] Previ- ous approaches to this task included keyword-matching, weighted keyword matching, and vector-based representations based on the occurrence of words in documents.\n",
            "[(0, 1.0024028267550074), (1, -0.3145353992376818)] LSA extends the vector-based approach by using Singular Value Decomposition (SVD) to reconﬁgure the data.\n",
            "[(0, 0.3247816769527303), (1, 0.11758144350316897)] The details of this pro- cess are described below, but the intuition is that there is a set of underlying latent variables which spans the meanings that can be expressed in a partic- ular language.\n",
            "[(0, 0.22462695771197955), (1, -0.13739930690079039)] These variables are assumed to be independent (and therefore orthogonal in the vector space).\n",
            "[(0, 0.3106921735336944), (1, -0.20576715983373164)] SVD is a matrix algebra technique which essentially re-orients and ranks the dimensions in a vector space.\n",
            "[(0, 0.6399448747845101), (1, -0.5145641967770594)] Because the dimensions in a vector space computed by SVD are ordered from most to least signiﬁcant, if some of the less signiﬁcant dimensions are 2ignored, the reduced representation is guaranteed to be the best possible for that dimensionality.\n",
            "[(0, 0.8946941013589998), (1, -0.26698088048588764)] In LSA, the typical assumption is that only the top 300 or so dimensions (out of tens or even hundreds of thousands) are useful for capturing the meaning of texts.\n",
            "[(0, 1.3508473230275055), (1, -1.2791258906117757)] By basing the representations on a reduced number of dimensions, words that generally occur in similar contexts have similar vectors and will therefore get a high similarity rating.\n",
            "[(0, 0.050739543521704907), (1, -0.053130581038527476)] The discarded dimensions are assumed to be the product of noise, random associations, or some other non-essential factor.\n",
            "[(0, 0.6574710828856902), (1, -0.19947186172213036)] That LSA performed information retrieval better than its rival approaches is not especially surprising.\n",
            "[(0, 0.29872716730829063), (1, -0.10269668030161502)] What is more surprising is how well it models human behavior on a variety of linguistic tasks.\n",
            "[(0, 0.7062064116105634), (1, 0.015003665963833589)] Before describing these, however, the LSA technique is described in more detail.\n",
            "[(0, 0.29562537777610887), (1, -0.2310376747491267)] Although there are some variations, the most common steps are these: •Collect a large set of (domain-relevant) text and separate it into “docu- ments”.\n",
            "[(0, 0.5659494058832093), (1, 0.03608899660629344)] For most applications, each paragraph is treated as a separate document based on the intuition that the information within a para- graph tends to be coherent and related.\n",
            "[(0, 0.4389989060576951), (1, -0.047169064860907314)] •Next, create a co-occurrence matrix of documents and terms.\n",
            "[(0, 0.44564561629004085), (1, -0.05987249756322154)] The cell in this matrix corresponding to document xand term ycontains the number of times yoccurs in x.\n",
            "[(0, 1.437305966873764), (1, -1.1349157976563755)] A term is deﬁned as a word which occurs in more than one document, and no stemming or other morphological analysis is performed to try to combine diﬀerent forms of the same word.\n",
            "[(0, 0.8657510155570832), (1, -0.3221555992546767)] 3If there are mterms and ndocuments, this matrix can be viewed as giving a representation which has an m-dimensional vector for each document, and an n-dimensional vector for each term.\n",
            "[(0, 0.7627448068735893), (1, -0.787744843004525)] •The values in each cell may be weighted to reduce the eﬀect of common words that occur through the corpus.\n",
            "[(0, 0.2645812456195567), (1, -0.1608976123922754)] A common weighting method is “log entropy”, based on Information Theory, which multiplies the value by its information gain.\n",
            "[(0, 0.1322771205558614), (1, -0.1298737747825511)] •SVD is invoked with a parameter kwhich speciﬁes the desired number of dimensions.\n",
            "[(0, 0.2181146314612665), (1, -0.15681090717245355)] (In principle, the SVD would be computed with all the dimensions to create three matrices that, when multiplied together, would produce the original data, but due to the amount of memory that this would require, this is not feasible.\n",
            "[(0, 0.32384922243768355), (1, -0.020931692270592384)] Instead, the currently used algorithms are optimized for dealing with sparse data spaces and compute only the most signiﬁcant kdimensions of the matrices.)\n",
            "[(0, 0.03745365894841195), (1, -0.014801368719640131)] The result of the processing above is three matrices.\n",
            "[(0, 1.0722060930936534), (1, -0.5420060807726842)] One has a k- dimensional vector for each document, one has a k-dimensional vector for each term in the corpus, and one has the ksingular values.\n",
            "[(0, 0.565390594522723), (1, -0.4612806996801935)] The ﬁrst two matrices deﬁne two diﬀerent vector spaces which are also diﬀerent from the space deﬁned by the original matrix.\n",
            "[(0, 0.44393975653532153), (1, -0.14890038081147622)] The singular values can be used to transform a vector from one space to the other.\n",
            "[(0, 0.23958952623610325), (1, 0.10051386498440733)] The use of these matrices depends on the application.\n",
            "[(0, 1.2417798964632636), (1, -0.10433684077957539)] For information retrieval, the document vectors contain the LSA repre- sentation of each document.\n",
            "[(0, 0.5695099854007558), (1, -0.3453863708277611)] A query is turned into a “pseudodoc” in the 4document vector space by combining the vectors for the terms in the query, and dividing by the singular values.\n",
            "[(0, 0.25010653935294236), (1, -0.005762967584685935)] Vectors are typically compared by com- puting the cosine between them.\n",
            "[(0, 0.23284250639841794), (1, 0.10905756284033104)] (Some applications use other distance met- rics.)\n",
            "[(0, 1.4910660614278182), (1, -0.2472131281415765)] The closest vectors from the document vector space correspond to the documents which are closest in meaning to the query (according to LSA).\n",
            "[(0, 0.7148989655632758), (1, 0.05472055175675232)] In most other applications, the original documents are only used for train- ing, that is, creating the semantic space.\n",
            "[(0, 0.5041990963973281), (1, -0.05225957446439671)] To compare new texts, the term vectors are combined as described above.\n",
            "[(0, 0.39896188955505224), (1, -0.1023711744386759)] Here, no manipulation with the singular values is required because the vectors are compared in the term space.\n",
            "[(0, 0.9451572292619284), (1, 0.38786541858296464)] For more details about the mathematical foundations of LSA, see Golub (1989) and Hu (2005).\n",
            "[(0, 1.0311700781177466), (1, 0.45460615144812916)] For more details about the creation of LSA spaces, see Deerwester (1990) and Quesada (2005).\n",
            "[(0, 1.2032988406040208), (1, -0.40514297521591536)] 3 Support for LSA Support for LSA might be said to stem from the time of World War II when Wittgenstein wrote (what was later translated as): .\n",
            "[] .\n",
            "[] .\n",
            "[(0, 1.296104340686351), (1, -0.9589634168087886)] for a large class of cases — though not for all — in which we employ the word “meaning” it can be deﬁned as thus, the meaning of the word is its use in language.\n",
            "[(0, 3.79724270375292), (1, -2.2023243304969937)] (Wittgenstein, 1958, p.\n",
            "20) 5There have been a large number of psychological studies which have taken Wittgenstein’s words to heart, and shown that LSA’s behavior is closely matched with that of humans, for example: •LSA acquires words at a similar pace to human children, sometimes exceeding the number of words to which it is exposed (see Landauer and Dumais, 1997) .\n",
            "[(0, 1.1400378759912824), (1, 0.3105314557931747)] •LSA’s knowledge of synonyms is as good as that of second-language English speakers as evidenced by scores on the Test of English as a Foreign Language (TOEFL, see Landauer, 1997) .\n",
            "[(0, 1.033969481853749), (1, 0.6978144946301518)] •LSA can tell students what they should read next to help them learn (see Wolfe, 1998).\n",
            "[(0, 0.801305604470058), (1, 0.2306219320634319)] •LSA can even interpret metaphors like, “My lawyer is a shark” (see Kintsch, 2001).\n",
            "[(0, 0.7602043494770024), (1, -0.18979235678197867)] For textual applications, LSA has another beneﬁt besides its high cor- relation with human behavior.\n",
            "[(0, 1.3934374450445515), (1, -0.25215408104043574)] When compared with the traditional labor- intensive approach to Natural Language Processing — developing a gram- mar, a lexicon, a semantic representation and the processing engine needed to combine them — developing an LSA-based representation is quite simple.\n",
            "[(0, 0.03125240779534714), (1, -0.0351725134914937)] It also has the advantage of graceful degradation.\n",
            "[(0, 1.5724491971808008), (1, -1.2131115615672217)] If it doesn’t know a word, LSA simply ignores it and bases its representation on the other words.\n",
            "[(0, 5.902721989878568), (1, 7.573413969549232)] This has led researchers to use LSA for a variety of applications, including: 6•intelligent tutoring systems which allow students to enter unconstrained natural language replies to questions (see Graesser, 2000 and Wiemer- Hastings, 2004), •grading psychology essays by comparing them to pre-graded essays (see Foltz, 1996) , •evaluating summaries of documents to help teach summarization skills (Summary Street, described at http://colit.org/), •helping students learn to properly integrate and cite material from multiple documents (see Britt, 2005), and •evaluating airplane landings in a ﬂight simulator (see Quesada, 2005).\n",
            "[(0, 1.3557480856971749), (1, -0.6930779373123216)] The only applicability constraints for LSA are that the task is text-based, it can be framed in terms of computing the similarity of texts, and there is an available training corpus.\n",
            "[(0, 0.4711494557791096), (1, 0.5927093977331039)] The tutoring systems, for example, compare a student’s answer for a question to a set of expected answers.\n",
            "[(0, 0.3337187864130386), (1, 0.42763498891940493)] If the student’s response is close enough to a good answer, then the system gives positive feedback and moves on to the next question.\n",
            "[(0, 0.3831189874461264), (1, 0.5456087165550145)] If the student’s answer matches an expected bad answer, then the system steers the student back on track.\n",
            "[(0, 0.6317908881584432), (1, -0.0762730326028412)] 4 Issues The research issues facing the LSA community range from the practical to the philosophical.\n",
            "[(0, 0.29246523346030306), (1, -0.11119680269012008)] One basic question addresses the size and substance of the 7training corpus.\n",
            "[(0, 0.6865652292017125), (1, -0.12874916958334798)] Many eﬀective LSA applications have been developed us- ing relatively small corpora.\n",
            "[(0, 2.1915563917111336), (1, -1.200905606244997)] In one of the successful applications mentioned above, LSA was trained on a corpus of only a couple hundred kilobytes with 2000 word types, 30,000 word tokens, and 325 documents.\n",
            "[(0, 2.120854648201684), (1, -1.283822071921389)] In contrast, re- searchers at the University of Colorado have reported that they have trained LSA on a corpus containing 750,000 word types, 550 million word tokens, and 3.6 million documents.\n",
            "[(0, 0.6663490806720213), (1, -0.2902073669239415)] Unfortunately, there is little hard evidence on what the “ideal” size of an LSA corpus might be.\n",
            "[(0, 1.6633724125061045), (1, -0.7557815802350614)] The current data suggests that adding additional texts is unlikely to reduce performance, so a basic rule of thumb is, “the more the better.” The obvious follow-up question is, “What kinds of text should be included in an LSA corpus?” The common wisdom holds that the corpus should consist of texts which are relevant to the particular target task.\n",
            "[(0, 0.5953825386019599), (1, -0.45864114163251773)] The domain can deﬁne a sub-language where words are interpreted in consistent ways.\n",
            "[(0, 0.5560878305251236), (1, -0.426565711332849)] Furthermore, a primary concern is to achieve suﬃcient coverage of the words which will be encountered in the course of running the application.\n",
            "[(0, 1.3227136028212618), (1, -1.1903127772408277)] A domain- speciﬁc corpus will have a higher percentage of relevant words and will thus not waste its “representational power” on words that will not be seen by the application.\n",
            "[(0, 1.2969885943285426), (1, -0.861623564806408)] One critical objection that is raised against the LSA approach is that not only does it ignore the syntactic structure of sentences, it even ignores word order.\n",
            "[(0, 1.4144712581832803), (1, -1.0673153487795792)] In other words, LSA treats a text as a bag of words.\n",
            "[(0, 2.716209441682509), (1, -2.088245788656201)] In practice, LSA does well with longer passages of words (deﬁned by Rehder (1998) as 8more than 200 words ) where syntactic details may be “washed out”, and it also does well with single words (the TOEFL test, for example), but it does not do well on single sentences as shown by Wiemer-Hastings (1999) .\n",
            "[(0, 1.7343874898780196), (1, 0.9447840534606686)] There have been a variety of approaches which attempt to deal with this, including surface parsing of sentences so that the components can be compared sepa- rately with LSA and using String Edit Theory to infer structural relations (see Wiemer-Hastings, 2001, Kanejiya, 2003, and Dennis, 2005) .\n",
            "[(0, 0.514517234540102), (1, -0.145571850531214)] Another notable gap in LSA’s competence is negations.\n",
            "[(0, 2.043800876806583), (1, -1.0886155667631752)] thing that LSA “ignores” is negations, either because they are omitted from the LSA training via a “stop words” list, or simply because their widespread use throughout a corpus renders them representationally depleted.\n",
            "[(0, 0.311939548703594), (1, -0.017074511939043066)] Although no satisfactory approach yet exists for dealing with negations, a possibility would be to treat them as an essentially syntactic component that can processed as described above.\n",
            "[(0, 0.6355099205948491), (1, -0.11501420998177637)] A more fundamental question about LSA is what its dimensions “mean”.\n",
            "[(0, 0.030877999570868966), (1, -0.009372526290607431)] Because they represent latent variables, there is no clear deﬁnition.\n",
            "[(0, 0.7830454311125645), (1, -0.7761388108129849)] As shown by Hu (2003), there is a high correlation between the ﬁrst dimension and the frequency of occurrence of the words in the corpus .\n",
            "[(0, 0.015452040319991792), (1, 0.030522635617835688)] Beyond that, there are no clear answers.\n",
            "[(0, 0.8801484544865408), (1, -0.5207458149765403)] There is also considerable debate as to what extent LSA captures ﬁrst order co-occurrence or higher order co-occurrence.\n",
            "[(0, 2.2081071892056254), (1, -1.6019985730197277)] Recent evidence from Denhi` ere (2005) shows that although second order eﬀects do occur, large changes in the similarity measure between two words can be seen when a document is added to a training corpus in which both words occur (ﬁrst order co-occurrence) .\n",
            "[(0, 0.3236693428669267), (1, -0.07533522475715676)] 9LSA’s utility and correspondence with human behavor have made it a popular technique for psycholinguistic research and text processing.\n",
            "[(0, 0.764667690405409), (1, -0.014721413514857588)] The issues describe here (along with others, for example, “Is LSA at all psycho- logically plausible?”) will keeep researchers busy for years to come.\n",
            "[(0, 0.8533913731553561), (1, 0.0877359900882615)] A col- lection edited by McNamara (2005) provides considerably more detail about the current practice and research on LSA.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def takenext(elem):\n",
        "\t\"\"\"\n",
        "\tsort\n",
        "\t\"\"\"\n",
        "\treturn elem[1]"
      ],
      "metadata": {
        "id": "ZOcAoLuy-mT4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sort the vectors in descending order of weightage"
      ],
      "metadata": {
        "id": "Xk6V-gp--07b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#sort each vector by score\n",
        "vecsSort = list(map(lambda i: list(), range(2))) #output: [[ ], [ ]]\n",
        "for i,docv in enumerate(corpus_lsi):\n",
        "\tfor sc in docv:\n",
        "\t\tisent = (i, abs(sc[1]))\n",
        "\t\tvecsSort[sc[0]].append(isent)\n",
        "vecsSort = list(map(lambda x: sorted(x,key=takenext,reverse=True), vecsSort))"
      ],
      "metadata": {
        "id": "-tqq91TA-3Te"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(vecsSort)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WRh4uTnuACs_",
        "outputId": "15d56fef-0797-4427-9928-cce1e6aceab4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[(55, 5.902721989878568), (47, 3.79724270375292), (72, 2.716209441682509), (82, 2.2081071892056254), (63, 2.1915563917111336), (64, 2.120854648201684), (75, 2.043800876806583), (4, 2.0375292895239103), (3, 1.8813663627116441), (73, 1.7343874898780196), (66, 1.6633724125061045), (5, 1.5814615923125204), (54, 1.5724491971808008), (37, 1.4910660614278182), (21, 1.437305966873764), (71, 1.4144712581832803), (52, 1.3934374450445515), (56, 1.3557480856971749), (12, 1.3508473230275055), (69, 1.3227136028212618), (70, 1.2969885943285426), (46, 1.296104340686351), (33, 1.2417798964632636), (43, 1.2032988406040208), (0, 1.153840519596598), (48, 1.1400378759912824), (1, 1.1046627524030228), (29, 1.0722060930936534), (49, 1.033969481853749), (42, 1.0311700781177466), (6, 1.0024028267550074), (41, 0.9451572292619284), (11, 0.8946941013589998), (81, 0.8801484544865408), (22, 0.8657510155570832), (85, 0.8533913731553561), (50, 0.801305604470058), (79, 0.7830454311125645), (84, 0.764667690405409), (23, 0.7627448068735893), (51, 0.7602043494770024), (2, 0.7560582240652032), (38, 0.7148989655632758), (16, 0.7062064116105634), (62, 0.6865652292017125), (65, 0.6663490806720213), (14, 0.6574710828856902), (10, 0.6399448747845101), (77, 0.6355099205948491), (60, 0.6317908881584432), (67, 0.5953825386019599), (34, 0.5695099854007558), (18, 0.5659494058832093), (30, 0.565390594522723), (68, 0.5560878305251236), (74, 0.514517234540102), (39, 0.5041990963973281), (57, 0.4711494557791096), (20, 0.44564561629004085), (31, 0.44393975653532153), (19, 0.4389989060576951), (40, 0.39896188955505224), (59, 0.3831189874461264), (58, 0.3337187864130386), (7, 0.3247816769527303), (27, 0.32384922243768355), (83, 0.3236693428669267), (76, 0.311939548703594), (9, 0.3106921735336944), (15, 0.29872716730829063), (17, 0.29562537777610887), (61, 0.29246523346030306), (24, 0.2645812456195567), (35, 0.25010653935294236), (32, 0.23958952623610325), (36, 0.23284250639841794), (8, 0.22462695771197955), (26, 0.2181146314612665), (25, 0.1322771205558614), (13, 0.050739543521704907), (28, 0.03745365894841195), (53, 0.03125240779534714), (78, 0.030877999570868966), (80, 0.015452040319991792)], [(55, 7.573413969549232), (47, 2.2023243304969937), (72, 2.088245788656201), (82, 1.6019985730197277), (64, 1.283822071921389), (12, 1.2791258906117757), (54, 1.2131115615672217), (63, 1.200905606244997), (69, 1.1903127772408277), (21, 1.1349157976563755), (75, 1.0886155667631752), (71, 1.0673153487795792), (46, 0.9589634168087886), (73, 0.9447840534606686), (70, 0.861623564806408), (5, 0.8565792965495629), (23, 0.787744843004525), (79, 0.7761388108129849), (66, 0.7557815802350614), (49, 0.6978144946301518), (56, 0.6930779373123216), (57, 0.5927093977331039), (0, 0.5633294910394763), (59, 0.5456087165550145), (29, 0.5420060807726842), (81, 0.5207458149765403), (10, 0.5145641967770594), (30, 0.4612806996801935), (67, 0.45864114163251773), (42, 0.45460615144812916), (1, 0.43781962012493486), (58, 0.42763498891940493), (68, 0.426565711332849), (43, 0.40514297521591536), (3, 0.3990646271206591), (41, 0.38786541858296464), (4, 0.37372896615587753), (34, 0.3453863708277611), (22, 0.3221555992546767), (6, 0.3145353992376818), (48, 0.3105314557931747), (65, 0.2902073669239415), (2, 0.2889067628937494), (11, 0.26698088048588764), (52, 0.25215408104043574), (37, 0.2472131281415765), (17, 0.2310376747491267), (50, 0.2306219320634319), (9, 0.20576715983373164), (14, 0.19947186172213036), (51, 0.18979235678197867), (24, 0.1608976123922754), (26, 0.15681090717245355), (31, 0.14890038081147622), (74, 0.145571850531214), (8, 0.13739930690079039), (25, 0.1298737747825511), (62, 0.12874916958334798), (7, 0.11758144350316897), (77, 0.11501420998177637), (61, 0.11119680269012008), (36, 0.10905756284033104), (33, 0.10433684077957539), (15, 0.10269668030161502), (40, 0.1023711744386759), (32, 0.10051386498440733), (85, 0.0877359900882615), (60, 0.0762730326028412), (83, 0.07533522475715676), (20, 0.05987249756322154), (38, 0.05472055175675232), (13, 0.053130581038527476), (39, 0.05225957446439671), (19, 0.047169064860907314), (18, 0.03608899660629344), (53, 0.0351725134914937), (80, 0.030522635617835688), (27, 0.020931692270592384), (76, 0.017074511939043066), (16, 0.015003665963833589), (28, 0.014801368719640131), (84, 0.014721413514857588), (78, 0.009372526290607431), (35, 0.005762967584685935)]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "3KYZI58yYSh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Select the sentences for the summary"
      ],
      "metadata": {
        "id": "MfMdOC5oApfx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentIndexes = set()"
      ],
      "metadata": {
        "id": "T0AVg_9EAGPC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def selectTopSent(summSize, numTopics, sortedVec):\n",
        "  topSentences = []\n",
        "  sent_no = []\n",
        "  sentInd = set()\n",
        "  sCount = 0\n",
        "  for i in range(summSize):\n",
        "    for j in range(numTopics):\n",
        "      vecs = sortedVec[j]\n",
        "      si = vecs[i][0]\n",
        "      if si not in sentInd:\n",
        "        sent_no.append(si)\n",
        "        topSentences.append(vecs[i])\n",
        "        sentInd.add(si)\n",
        "        sCount +=1\n",
        "        if sCount == summSize:\n",
        "          return sent_no"
      ],
      "metadata": {
        "id": "OoLYESBqAQPO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topSentences = selectTopSent(8, 2, vecsSort)"
      ],
      "metadata": {
        "id": "2nbANFxEAn5U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(topSentences)\n",
        "topSentences.sort()\n",
        "print(topSentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "usvEWDGwBC2t",
        "outputId": "7790b517-1cf9-4e5e-ae69-0aadb16c291b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[55, 47, 72, 82, 63, 64, 12, 75]\n",
            "[12, 47, 55, 63, 64, 72, 75, 82]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summary = []\n",
        "doc = []\n",
        "cnt = 0\n",
        "for sentence in document_list:\n",
        "  doc.append(sentence)\n",
        "  if cnt in topSentences:\n",
        "    summary.append(sentence)\n",
        "  cnt += 1\n",
        "summary = \" \".join(summary)\n",
        "doc = \" \".join(doc)\n",
        "print(\"\\n\")\n",
        "print(\"Original:\")\n",
        "print(doc)\n",
        "print(\"Summary:\")\n",
        "print(summary)\n",
        "print(\"Keywords:\")\n",
        "print(keywords)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XIArGeXKBZ31",
        "outputId": "90079e68-179f-41c4-c0a7-9f56b4a711a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Original:\n",
            "Latent Semantic Analysis (LSA) is a technique for creating vector-based representations of texts which are claimed to capture their semantic con- tent. The primary function of LSA is to compute the similarity of text pairs 1by comparing their vector representations. This relatively simple similarity metric has been situated within a psychological theory of text meaning and has been shown to closely match human capabilities on a variety of tasks. This article follows the developmental path of LSA, describing its histor- ical context, showing how it computes and uses its vector representations, and then giving examples of the theoretical and empirical support for LSA and its current research directions. 2 How LSA works LSA (originally known as Latent Semantic Indexing) was developed for the task of Information Retrieval, that is, selecting from a large database of documents a few relevant documents which match a given query . Previ- ous approaches to this task included keyword-matching, weighted keyword matching, and vector-based representations based on the occurrence of words in documents. LSA extends the vector-based approach by using Singular Value Decomposition (SVD) to reconﬁgure the data. The details of this pro- cess are described below, but the intuition is that there is a set of underlying latent variables which spans the meanings that can be expressed in a partic- ular language. These variables are assumed to be independent (and therefore orthogonal in the vector space). SVD is a matrix algebra technique which essentially re-orients and ranks the dimensions in a vector space. Because the dimensions in a vector space computed by SVD are ordered from most to least signiﬁcant, if some of the less signiﬁcant dimensions are 2ignored, the reduced representation is guaranteed to be the best possible for that dimensionality. In LSA, the typical assumption is that only the top 300 or so dimensions (out of tens or even hundreds of thousands) are useful for capturing the meaning of texts. By basing the representations on a reduced number of dimensions, words that generally occur in similar contexts have similar vectors and will therefore get a high similarity rating. The discarded dimensions are assumed to be the product of noise, random associations, or some other non-essential factor. That LSA performed information retrieval better than its rival approaches is not especially surprising. What is more surprising is how well it models human behavior on a variety of linguistic tasks. Before describing these, however, the LSA technique is described in more detail. Although there are some variations, the most common steps are these: •Collect a large set of (domain-relevant) text and separate it into “docu- ments”. For most applications, each paragraph is treated as a separate document based on the intuition that the information within a para- graph tends to be coherent and related. •Next, create a co-occurrence matrix of documents and terms. The cell in this matrix corresponding to document xand term ycontains the number of times yoccurs in x. A term is deﬁned as a word which occurs in more than one document, and no stemming or other morphological analysis is performed to try to combine diﬀerent forms of the same word. 3If there are mterms and ndocuments, this matrix can be viewed as giving a representation which has an m-dimensional vector for each document, and an n-dimensional vector for each term. •The values in each cell may be weighted to reduce the eﬀect of common words that occur through the corpus. A common weighting method is “log entropy”, based on Information Theory, which multiplies the value by its information gain. •SVD is invoked with a parameter kwhich speciﬁes the desired number of dimensions. (In principle, the SVD would be computed with all the dimensions to create three matrices that, when multiplied together, would produce the original data, but due to the amount of memory that this would require, this is not feasible. Instead, the currently used algorithms are optimized for dealing with sparse data spaces and compute only the most signiﬁcant kdimensions of the matrices.) The result of the processing above is three matrices. One has a k- dimensional vector for each document, one has a k-dimensional vector for each term in the corpus, and one has the ksingular values. The ﬁrst two matrices deﬁne two diﬀerent vector spaces which are also diﬀerent from the space deﬁned by the original matrix. The singular values can be used to transform a vector from one space to the other. The use of these matrices depends on the application. For information retrieval, the document vectors contain the LSA repre- sentation of each document. A query is turned into a “pseudodoc” in the 4document vector space by combining the vectors for the terms in the query, and dividing by the singular values. Vectors are typically compared by com- puting the cosine between them. (Some applications use other distance met- rics.) The closest vectors from the document vector space correspond to the documents which are closest in meaning to the query (according to LSA). In most other applications, the original documents are only used for train- ing, that is, creating the semantic space. To compare new texts, the term vectors are combined as described above. Here, no manipulation with the singular values is required because the vectors are compared in the term space. For more details about the mathematical foundations of LSA, see Golub (1989) and Hu (2005). For more details about the creation of LSA spaces, see Deerwester (1990) and Quesada (2005). 3 Support for LSA Support for LSA might be said to stem from the time of World War II when Wittgenstein wrote (what was later translated as): . . . for a large class of cases — though not for all — in which we employ the word “meaning” it can be deﬁned as thus, the meaning of the word is its use in language. (Wittgenstein, 1958, p.\n",
            "20) 5There have been a large number of psychological studies which have taken Wittgenstein’s words to heart, and shown that LSA’s behavior is closely matched with that of humans, for example: •LSA acquires words at a similar pace to human children, sometimes exceeding the number of words to which it is exposed (see Landauer and Dumais, 1997) . •LSA’s knowledge of synonyms is as good as that of second-language English speakers as evidenced by scores on the Test of English as a Foreign Language (TOEFL, see Landauer, 1997) . •LSA can tell students what they should read next to help them learn (see Wolfe, 1998). •LSA can even interpret metaphors like, “My lawyer is a shark” (see Kintsch, 2001). For textual applications, LSA has another beneﬁt besides its high cor- relation with human behavior. When compared with the traditional labor- intensive approach to Natural Language Processing — developing a gram- mar, a lexicon, a semantic representation and the processing engine needed to combine them — developing an LSA-based representation is quite simple. It also has the advantage of graceful degradation. If it doesn’t know a word, LSA simply ignores it and bases its representation on the other words. This has led researchers to use LSA for a variety of applications, including: 6•intelligent tutoring systems which allow students to enter unconstrained natural language replies to questions (see Graesser, 2000 and Wiemer- Hastings, 2004), •grading psychology essays by comparing them to pre-graded essays (see Foltz, 1996) , •evaluating summaries of documents to help teach summarization skills (Summary Street, described at http://colit.org/), •helping students learn to properly integrate and cite material from multiple documents (see Britt, 2005), and •evaluating airplane landings in a ﬂight simulator (see Quesada, 2005). The only applicability constraints for LSA are that the task is text-based, it can be framed in terms of computing the similarity of texts, and there is an available training corpus. The tutoring systems, for example, compare a student’s answer for a question to a set of expected answers. If the student’s response is close enough to a good answer, then the system gives positive feedback and moves on to the next question. If the student’s answer matches an expected bad answer, then the system steers the student back on track. 4 Issues The research issues facing the LSA community range from the practical to the philosophical. One basic question addresses the size and substance of the 7training corpus. Many eﬀective LSA applications have been developed us- ing relatively small corpora. In one of the successful applications mentioned above, LSA was trained on a corpus of only a couple hundred kilobytes with 2000 word types, 30,000 word tokens, and 325 documents. In contrast, re- searchers at the University of Colorado have reported that they have trained LSA on a corpus containing 750,000 word types, 550 million word tokens, and 3.6 million documents. Unfortunately, there is little hard evidence on what the “ideal” size of an LSA corpus might be. The current data suggests that adding additional texts is unlikely to reduce performance, so a basic rule of thumb is, “the more the better.” The obvious follow-up question is, “What kinds of text should be included in an LSA corpus?” The common wisdom holds that the corpus should consist of texts which are relevant to the particular target task. The domain can deﬁne a sub-language where words are interpreted in consistent ways. Furthermore, a primary concern is to achieve suﬃcient coverage of the words which will be encountered in the course of running the application. A domain- speciﬁc corpus will have a higher percentage of relevant words and will thus not waste its “representational power” on words that will not be seen by the application. One critical objection that is raised against the LSA approach is that not only does it ignore the syntactic structure of sentences, it even ignores word order. In other words, LSA treats a text as a bag of words. In practice, LSA does well with longer passages of words (deﬁned by Rehder (1998) as 8more than 200 words ) where syntactic details may be “washed out”, and it also does well with single words (the TOEFL test, for example), but it does not do well on single sentences as shown by Wiemer-Hastings (1999) . There have been a variety of approaches which attempt to deal with this, including surface parsing of sentences so that the components can be compared sepa- rately with LSA and using String Edit Theory to infer structural relations (see Wiemer-Hastings, 2001, Kanejiya, 2003, and Dennis, 2005) . Another notable gap in LSA’s competence is negations. thing that LSA “ignores” is negations, either because they are omitted from the LSA training via a “stop words” list, or simply because their widespread use throughout a corpus renders them representationally depleted. Although no satisfactory approach yet exists for dealing with negations, a possibility would be to treat them as an essentially syntactic component that can processed as described above. A more fundamental question about LSA is what its dimensions “mean”. Because they represent latent variables, there is no clear deﬁnition. As shown by Hu (2003), there is a high correlation between the ﬁrst dimension and the frequency of occurrence of the words in the corpus . Beyond that, there are no clear answers. There is also considerable debate as to what extent LSA captures ﬁrst order co-occurrence or higher order co-occurrence. Recent evidence from Denhi` ere (2005) shows that although second order eﬀects do occur, large changes in the similarity measure between two words can be seen when a document is added to a training corpus in which both words occur (ﬁrst order co-occurrence) . 9LSA’s utility and correspondence with human behavor have made it a popular technique for psycholinguistic research and text processing. The issues describe here (along with others, for example, “Is LSA at all psycho- logically plausible?”) will keeep researchers busy for years to come. A col- lection edited by McNamara (2005) provides considerably more detail about the current practice and research on LSA.\n",
            "Summary:\n",
            "By basing the representations on a reduced number of dimensions, words that generally occur in similar contexts have similar vectors and will therefore get a high similarity rating. (Wittgenstein, 1958, p.\n",
            "20) 5There have been a large number of psychological studies which have taken Wittgenstein’s words to heart, and shown that LSA’s behavior is closely matched with that of humans, for example: •LSA acquires words at a similar pace to human children, sometimes exceeding the number of words to which it is exposed (see Landauer and Dumais, 1997) . This has led researchers to use LSA for a variety of applications, including: 6•intelligent tutoring systems which allow students to enter unconstrained natural language replies to questions (see Graesser, 2000 and Wiemer- Hastings, 2004), •grading psychology essays by comparing them to pre-graded essays (see Foltz, 1996) , •evaluating summaries of documents to help teach summarization skills (Summary Street, described at http://colit.org/), •helping students learn to properly integrate and cite material from multiple documents (see Britt, 2005), and •evaluating airplane landings in a ﬂight simulator (see Quesada, 2005). In one of the successful applications mentioned above, LSA was trained on a corpus of only a couple hundred kilobytes with 2000 word types, 30,000 word tokens, and 325 documents. In contrast, re- searchers at the University of Colorado have reported that they have trained LSA on a corpus containing 750,000 word types, 550 million word tokens, and 3.6 million documents. In practice, LSA does well with longer passages of words (deﬁned by Rehder (1998) as 8more than 200 words ) where syntactic details may be “washed out”, and it also does well with single words (the TOEFL test, for example), but it does not do well on single sentences as shown by Wiemer-Hastings (1999) . thing that LSA “ignores” is negations, either because they are omitted from the LSA training via a “stop words” list, or simply because their widespread use throughout a corpus renders them representationally depleted. Recent evidence from Denhi` ere (2005) shows that although second order eﬀects do occur, large changes in the similarity measure between two words can be seen when a document is added to a training corpus in which both words occur (ﬁrst order co-occurrence) .\n",
            "Keywords:\n",
            "['lsa', 'word', 'vector', 'document', 'corpu']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Hnyb3IwmBdIm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}